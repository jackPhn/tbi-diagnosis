{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6722e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import cmath\n",
    "import multiprocessing\n",
    "import random\n",
    "import time\n",
    "import matplotlib\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K \n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "import h5py\n",
    "from datetime import datetime\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard, LambdaCallback\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    MaxPooling2D,\n",
    "    UpSampling2D,\n",
    "    Dropout\n",
    ")\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from scipy.io import loadmat\n",
    "# Extract axis information to produce cone-shape images\n",
    "def extract_axis(datapath, axisPath):\n",
    "    data = loadmat(datapath)\n",
    "\n",
    "    xaxis = np.array(list(data['xAxis']))\n",
    "    yaxis = np.array(list(data['zAxis']))\n",
    "\n",
    "    xaxis = cv2.resize(xaxis, (80, 256), interpolation=cv2.INTER_AREA)\n",
    "    yaxis = cv2.resize(yaxis, (80, 256), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    xaxis += 100\n",
    "    yaxis -= 4\n",
    "\n",
    "    print(\"saved axis info in : {}\".format(axisPath))\n",
    "    np.save(os.path.join(axisPath, \"xAxis.npy\"), xaxis)\n",
    "    np.save(os.path.join(axisPath, \"yAxis.npy\"), yaxis)\n",
    "    \n",
    "    return xaxis, yaxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28fca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract generic axis information\n",
    "axisPath = \"/DATA/phan92/notebooks\"\n",
    "rand_input_file = \"/DATA/phan92/test_resource/raw_data/DoD110/DoD110_Ter002_LO1_Displacement_Normalized_3.mat\"\n",
    "xAxis, yAxis = extract_axis(rand_input_file, axisPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5238bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label, objective='skull'):\n",
    "    \"\"\"\n",
    "    Create one hot label from the provided label\n",
    "    \n",
    "    Args:\n",
    "        label: label of shape (N, x_dim, y_dim)\n",
    "        objective (string): objective of the model. brain, skull, or bleed\n",
    "        \n",
    "    Returns:\n",
    "        label: onehot label (N, x_dim, y_dim, n_channels)\n",
    "    \"\"\"\n",
    "    if objective == 'brain':\n",
    "        # create label to find brain tissue\n",
    "        # if the probability of brain is > 0, set the label to 1, 0 otherwise\n",
    "        label = np.where(label > 0, 1, 0)\n",
    "    elif objective == 'skull':\n",
    "        # create label to find the skull\n",
    "        # if the probability of skull is >= 0.5\n",
    "        # set the label to 1, 0 otherwise\n",
    "        label = np.where(label >= 1.5, 1, 0)\n",
    "    elif objective == 'bleed':\n",
    "        # create label to find bleed\n",
    "        # if the probability of bleed is >= 0.3\n",
    "        # set the label to 1\n",
    "        label = np.where(label >= 2.3, 1, 0)\n",
    "    else:\n",
    "        raise ValueError(\"No objective with name \\\"{name}\\\"\".format(name=objective))\n",
    "\n",
    "    label = np.expand_dims(label, axis=3)\n",
    "    #label = np.moveaxis(label, 3, 1)\n",
    "    \n",
    "    return label.astype(int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02831fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Read saved data and provide it to the model in batches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, group: h5py.Group, batch_size=32):\n",
    "        self.x_dset: h5py.Dataset = group['x']\n",
    "        self.y_dset: h5py.Dataset = group['y']\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the numner of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(self.x_dset.shape[0] / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, batch_index:int):\n",
    "        \"\"\"\n",
    "        Generate one batch of data\n",
    "        :param batch_index: index of this batch\n",
    "        :return: the data x and label y of the batch\n",
    "        \"\"\"\n",
    "        batch_start = batch_index * self.batch_size\n",
    "        batch_end = batch_start + self.batch_size\n",
    "\n",
    "        x = self.x_dset[batch_start : batch_end]\n",
    "        y = one_hot(self.y_dset[batch_start : batch_end])\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f78fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred, epsilon = 0.00001):\n",
    "    \"\"\"\n",
    "    Compute mean dice coefficient over all class\n",
    "    \n",
    "    Args:\n",
    "        y_true (Tensorflow tensor): tensor of ground truth\n",
    "                                    shape: (x_dim, y_dim, num_class)\n",
    "        y_pred (Tensorflow tensor): tensor of soft prediction for all classes\n",
    "                                    shape: (x_dim, y_dim, num_class)\n",
    "        epsilon (float): small constant added to avoid divide by 0 error\n",
    "    Return:\n",
    "        dice_coefficient (float): computed value of dice coefficient\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    dice_numerator = 2. * K.sum(y_true_f * y_pred_f, axis=-1) + epsilon\n",
    "    dice_denominator = K.sum(y_true_f, axis=-1) + K.sum(y_pred_f, axis=-1) + epsilon\n",
    "    dice_coefficient = K.mean(dice_numerator / dice_denominator)\n",
    "    \n",
    "    return dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred, epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63860b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_dice_loss(y_true, y_pred, epsilon=0.00001):\n",
    "    \"\"\"\n",
    "    Compute mean soft dice loss over all classes\n",
    "    Soft dice loss operate on float probability output\n",
    "    \n",
    "    Args:\n",
    "        y_true (Tensorflow tensor): tensor of ground truth\n",
    "                                    shape: (x_dim, y_dim, num_class)\n",
    "        y_pred (Tensorflow tensor): tensor of soft prediction for all classes\n",
    "                                    shape: (x_dim, y_dim, num_class)\n",
    "        epsilon (float): small constant added to avoid divide by 0 error\n",
    "        \n",
    "    Return:\n",
    "        dice_loss (float): computed value of dice loss\n",
    "    \"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    dice_numerator = 2. * K.sum(y_true_f * y_pred_f, axis=-1) + epsilon\n",
    "    dice_denominator = K.sum(y_true_f**2, axis=-1) + K.sum(y_pred_f**2, axis=-1) + epsilon\n",
    "    dice_loss = 1. - K.mean(dice_numerator / dice_denominator)\n",
    "    \n",
    "    return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_model(input_layer, start_neurons):\n",
    "    # 128 -> 64\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(0.25)(pool1)\n",
    "\n",
    "    # 64 -> 32\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
    "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    # 32 -> 16\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
    "    conv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(0.5)(pool3)\n",
    "\n",
    "    # 16 -> 8\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(pool3)\n",
    "    conv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(0.5)(pool4)\n",
    "\n",
    "    # Middle\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(pool4)\n",
    "    convm = Conv2D(start_neurons * 16, (3, 3), activation=\"relu\", padding=\"same\")(convm)\n",
    "\n",
    "    # 8 -> 16\n",
    "    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "    uconv4 = concatenate([deconv4, conv4])\n",
    "    uconv4 = Dropout(0.5)(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation=\"relu\", padding=\"same\")(uconv4)\n",
    "\n",
    "    # 16 -> 32\n",
    "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding=\"same\")(uconv4)\n",
    "    uconv3 = concatenate([deconv3, conv3])\n",
    "    uconv3 = Dropout(0.5)(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(uconv3)\n",
    "\n",
    "    # 32 -> 64\n",
    "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(uconv3)\n",
    "    uconv2 = concatenate([deconv2, conv2])\n",
    "    uconv2 = Dropout(0.5)(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
    "\n",
    "    # 64 -> 128\n",
    "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "    uconv1 = concatenate([deconv1, conv1])\n",
    "    uconv1 = Dropout(0.5)(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
    "\n",
    "    #uconv1 = Dropout(0.5)(uconv1)\n",
    "    output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv1)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, hdf5_file: str, checkpoint_dir: str, log_dir: str, epochs=50):\n",
    "        \"\"\"\n",
    "        Trains UNet model with data contained in given HDF5 file and saves\n",
    "        trained model to the checkpoint directory after each epoch\n",
    "        \n",
    "        Args:\n",
    "            hdf5_file: Path of hdf5 file which contains the dataset\n",
    "            checkpoint_dir (str): Directory where checkpoints are saved\n",
    "            log_dir (str): Directory where logs are saved\n",
    "            epochs (int): number of epochs\n",
    "        \"\"\"\n",
    "        dataset = h5py.File(hdf5_file, 'r')\n",
    "        \n",
    "        training_generator = DataGenerator(dataset['dev'])\n",
    "        validation_generator = DataGenerator(dataset['test'])\n",
    "        \n",
    "        # callback for tensorboard logs\n",
    "        log_dir = os.path.join(log_dir, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        os.makedirs(log_dir)\n",
    "        tb_callback = TensorBoard(log_dir=log_dir, write_images=True)\n",
    "        \n",
    "        # callback to save trained model weights\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        os.makedirs(checkpoint_dir)\n",
    "        weights_file = os.path.join(checkpoint_dir, 'unet.weights.epoch_{epoch:02d}.hdf5')\n",
    "        checkpoint = ModelCheckpoint(weights_file, verbose=1)\n",
    "        \n",
    "        model.fit(training_generator,\n",
    "                  validation_data=validation_generator,\n",
    "                  callbacks=[tb_callback, checkpoint],\n",
    "                  epochs=epochs)\n",
    "        \n",
    "        dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(num_folds, hdf5_file, epochs=50):\n",
    "    \"\"\"\n",
    "    Perform kfold cross validation on the training set\n",
    "    \n",
    "    Args:\n",
    "        num_folds (int): k in kfold\n",
    "        hdf5 (string): path to hdf5 dataset\n",
    "        epochs (int): number of epochs to train for\n",
    "        \n",
    "    Returns:\n",
    "        loss_per_fold: list of loss values\n",
    "        dice_per_fold: list of dice coefficient\n",
    "    \"\"\"\n",
    "    dice_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    \n",
    "    # get data\n",
    "    f = h5py.File(hdf5_file, 'r')\n",
    "    dev = f['dev']\n",
    "    x = dev['x']\n",
    "    y = dev['y']\n",
    "    \n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    \n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(x, y):\n",
    "        \n",
    "        # build model\n",
    "        input_layer = Input((256, 80, 1))\n",
    "        output_layer = build_unet_model(input_layer, 16)\n",
    "        model = Model(input_layer, output_layer)\n",
    "        model.compile(optimizer='adam', loss=soft_dice_loss, metrics=[dice_coefficient])\n",
    "        \n",
    "        print(\"Training for fold\", fold_no, \"....\")\n",
    "        history = model.fit(x[train], y[train], batch_size=32, epochs=epochs)\n",
    "        score = model.evaluate(x[test], y[test])\n",
    "        print(\"Score for fold\", fold_no, \": Dice loss =\", score[0], \", Dice score = \", score[1])\n",
    "        loss_per_fold.append(score[0])\n",
    "        dice_per_fold.append(score[1])\n",
    "        \n",
    "        fold_no += 1\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    return loss_per_fold, dice_per_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f44cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(label, prediction, displacement, name, xAxis, yAxis):\n",
    "    \"\"\"\n",
    "    Display an input\n",
    "    \n",
    "    Args:\n",
    "        label (numpy.arr): the label\n",
    "        prediction (numpy.arr): the prediction\n",
    "        displacement (numpy.arr): displacement\n",
    "        name (string): name of the data file\n",
    "        xAxis (numpy.arr): numpy array contain x axis for display the cone\n",
    "        yAxis (numpy.arr): numpy array contain y axis for display the cone\n",
    "    \"\"\"\n",
    "    prediction = np.squeeze(np.squeeze(prediction, axis=-1), axis=0)\n",
    "    displacement = np.squeeze(displacement, axis=-1)\n",
    "    # display\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(24, 6))\n",
    "    fig.suptitle(name, fontsize=16)\n",
    "    # label\n",
    "    ax[0].pcolormesh(xAxis, -yAxis, label, shading='auto', cmap='magma')\n",
    "    ax[0].title.set_text(\"Ground Truth\")\n",
    "    # prediction\n",
    "    ax[1].pcolormesh(xAxis, -yAxis, prediction, shading='auto', cmap='magma')\n",
    "    ax[1].title.set_text(\"Prediction\")\n",
    "    # displacement\n",
    "    ax[2].pcolormesh(xAxis, -yAxis, displacement, shading='auto')\n",
    "    ax[2].title.set_text(\"Standardized Displacement\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62316ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction(model, dataset, sample_num):\n",
    "    x = dataset['x']\n",
    "    y = dataset['y']\n",
    "    names = dataset['filename']\n",
    "    name = names[sample_num]\n",
    "    sample = x[sample_num]\n",
    "    y_true = y[sample_num]\n",
    "    y_pred = model.predict(np.expand_dims(sample, axis=0))\n",
    "    visualize_result(y_true, y_pred, sample, name.decode('utf-8'), xAxis, yAxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff3d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_specific_prediction(file_name:str, data_dir, model):\n",
    "    \"\"\"\n",
    "    Make a prediction for a specific file\n",
    "    \n",
    "    Args:\n",
    "        file_name: name of the .mat file (utf-8)\n",
    "        data_dir: location of the saved dataset\n",
    "        model: a trained model\n",
    "    \"\"\"\n",
    "    data = h5py.File(data_dir)\n",
    "    name_ascii = file_name.encode('ascii')\n",
    "    canPredict = find_and_predict(data['test'], name_ascii, model)\n",
    "    if not canPredict:\n",
    "        canPredict = find_and_predict(data['dev'], name_ascii, model)\n",
    "    data.close()\n",
    "    if not canPredict:\n",
    "        print(file_name, \"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_predict(dataset, file_name, model):\n",
    "    \"\"\"\n",
    "    Make a prediction for a specific file\n",
    "    \n",
    "    Args:\n",
    "        file_name: name of the .mat file (ascii)\n",
    "        dataset: train or test data set (h5py dataset)\n",
    "        model: a trained model\n",
    "    \n",
    "    Returns:\n",
    "        True if the file exist, False otherwise\n",
    "    \"\"\"\n",
    "    names = np.array(dataset['filename'])\n",
    "    idx = np.where(names == file_name)[0]\n",
    "    if idx.shape[0] > 0:\n",
    "        idx = idx[0]\n",
    "        show_prediction(model, dataset, idx)\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45cd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input((256, 80, 1))\n",
    "output_layer = build_unet_model(input_layer, 16)\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(optimizer='adam', loss=soft_dice_loss, metrics=[dice_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8564ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01078382",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, \n",
    "      hdf5_file='/DATA/phan92/tbi_diagnosis/data/processed/displacement_data.hdf5',\n",
    "      checkpoint_dir='/DATA/phan92/model_checkpoints',\n",
    "      log_dir='/DATA/phan92/tensorflow_logging', \n",
    "      epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35578e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File('/DATA/phan92/tbi_diagnosis/data/processed/displacement_data.hdf5', 'r')\n",
    "validation_generator = DataGenerator(dataset['testing'])\n",
    "val_loss, val_dice = model.evaluate(validation_generator)\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99792f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_specific_prediction('DoD009_Ter001_RC1_Displacement_Normalized_3.mat',\n",
    "                         '/DATA/phan92/tbi_diagnosis/data/processed/displacement_data.hdf5',\n",
    "                         model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bf72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File('/DATA/phan92/tbi_diagnosis/data/processed/displacement_data.hdf5', 'r')\n",
    "show_prediction(model, dataset['testing'], 6)\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b65ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f029c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_2 = Input((256, 80, 1))\n",
    "output_layer_2 = build_unet_model(input_layer_2, 16)\n",
    "model_2 = Model(input_layer_2, output_layer_2)\n",
    "model_2.compile(optimizer='adam', loss=soft_dice_loss, metrics=[dice_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e53ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model_2, \n",
    "      hdf5_file='/DATA/phan92/tbi_diagnosis/data/processed/displacementNorm_data.hdf5',\n",
    "      checkpoint_dir='/DATA/phan92/model_checkpoints',\n",
    "      log_dir='/DATA/phan92/tensorflow_logging', \n",
    "      epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d413bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File('/DATA/phan92/tbi_diagnosis/data/processed/displacementNorm_data.hdf5', 'r')\n",
    "validation_generator = DataGenerator(dataset['test'])\n",
    "val_loss, val_dice = model_2.evaluate(validation_generator)\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf84082",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_specific_prediction('DoD009_Ter001_RC1_Displacement_Normalized_3.mat',\n",
    "                         '/DATA/phan92/tbi_diagnosis/data/processed/displacementNorm_data.hdf5',\n",
    "                         model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File('/DATA/phan92/tbi_diagnosis/data/processed/displacementNorm_data.hdf5', 'r')\n",
    "show_prediction(model_2, dataset['test'], 24)\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82536c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636304ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dice = cross_validation(10, '/DATA/phan92/tbi_diagnosis/data/processed/displacementNorm_data.hdf5', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dce297",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528dd307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
