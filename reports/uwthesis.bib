 
Bibtex (bib) file for the thesis style document.
 
@article{explosive,
	title = "Explosive blast neurotrauma",
	journal = "J Neurotrauma",
	author = "G, Ling and F, Bandak and R, Armonda and G, Grant and J, Ecklund",
   	doi = "10.1089/neu.2007.0484",
	month = jun,
	year = 2009} 

@misc{TBI,
	title = {Traumatic brain injury},
	howpublished  = {\url{https://www.hopkinsmedicine.org/health/conditions-and-diseases/traumatic-brain-injury}},
	note = "Accessed: 2022-02-20",
}

@article{hyder_impact_2007,
	title = {The impact of traumatic brain injuries: A global perspective},
	volume = {22},
	issn = {10538135, 18786448},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/NRE-2007-22502},
	doi = {10.3233/NRE-2007-22502},
	shorttitle = {The impact of traumatic brain injuries},
	pages = {341-353},
	number = {5},
	journal = {PubMed},
	author = {Hyder, Adnan A. and Wunderlich, Colleen A. and Puvanachandra, Prasanthi and Gururaj, G. and Kobusingye, Olive C.},
	editor = {Neufeld, Jacob A.},
	year = {2007},
	langid = {english},
}

@article{heit_imaging_2017,
	title = {Imaging of Intracranial Hemorrhage},
	volume = {19},
	issn = {2287-6391, 2287-6405},
	doi = {10.5853/jos.2016.00563},
	pages = {11-27},
	number = {1},
	journal = {Journal of Stroke},
	shortjournal = {J Stroke},
	author = {Heit, Jeremy J. and Iv, Michael and Wintermark, Max},
	year = {2017},
}

@misc{cranial_ult,
   	title = {Cranial Ultrasound},
	howpublished  = {\url{https://www.healthlinkbc.ca/tests-treatments-medications/medical-tests/cranial-ultrasound}},
	note = "Accessed: 2022-02-21"
}

@misc{trauma_care,
	title = {Trauma Care},
	howpublished = {\url{https://www.uwmedicine.org/specialties/emergency-medicine/trauma-care}},
	note = "Accessed: 2022-02-22"
}

@article{management_2000,
	title = {Management and prognosis of severe traumatic brain injury},
	author = "Brain Tumor Foundation Inc 2000"
}

@article{kucewicz_tissue_2008,
	title = {Tissue Pulsatility Imaging of Cerebral Vasoreactivity During Hyperventilation},
	volume = {34},
	issn = {03015629},
	doi = {10.1016/j.ultrasmedbio.2008.01.001},
	pages = {1200--1208},
	number = {8},
	journal= {Ultrasound in Medicine \& Biology},
	author = {Kucewicz, John C. and Dunmire, Barbrina and Giardino, Nicholas D. and Leotta, Daniel F. and Paun, Marla and Dager, Stephen R. and Beach, Kirk W.},
	year = {2008},
	langid = {english},
}

@book{hemodynamics,
   author = {Strandness and Summer},
   title = "Hemodynamics for Surgeons",
   publisher = "New York: Grune and Strattion",
   year = 1975}

@article{pulsatile_echo,
  author = {J Campbell and J Clark and D White and C Jenkins},
  title = "Pulsatile echo-encephalography",
  journal = "Acta Neurologica Scandinavica. Supplementum",
  volume = 45,
  year = 1970,
  pages = "1-57"}

@article{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	language = {en},
	urldate = {2022-02-24},
	journal = {arXiv:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	note = {arXiv: 1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{isola_image_image_2018,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	language = {en},
	urldate = {2022-02-24},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
}

@article{zhang_resnest_2020,
	title = {{ResNeSt}: {Split}-{Attention} {Networks}},
	shorttitle = {{ResNeSt}},
	url = {http://arxiv.org/abs/2004.08955},
	language = {en},
	urldate = {2022-02-24},
	journal = {arXiv:2004.08955 [cs]},
	author = {Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.08955},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}


@inproceedings{navab_u-net_2015,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	copyright = {Springer International Publishing Switzerland 2015},
	isbn = {978-3-319-24573-7},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {eng},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year = {2015},
	note = {ISSN: 0302-9743},
	keywords = {Convolutional Layer, Data Augmentation, Deep Network, Ground Truth Segmentation, Training Image},
	pages = {234--241},
}

@article{cicek_3d_2016,
	title = {{3D} {U}-{Net}: {Learning} {Dense} {Volumetric} {Segmentation} from {Sparse} {Annotation}},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:1606.06650 [cs]},
	author = {Çiçek, Özgün and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.06650},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Conditionally accepted for MICCAI 2016},
}

@article{milletari_v-net_2016,
	title = {V-{Net}: {Fully} {Convolutional} {Neural} {Networks} for {Volumetric} {Medical} {Image} {Segmentation}},
	shorttitle = {V-{Net}},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:1606.04797 [cs]},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04797},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
}

@article{alom_recurrent_nodate,
	title = {Recurrent {Residual} {Convolutional} {Neural} {Network} based on {U}-{Net} ({R2U}-{Net}) for {Medical} {Image} {Segmentation}},
	language = {en},
	journal = {arXiv},
	author = {Alom, Zahangir and Taha, Tarek M and Asari, Vijayan K},
	pages = {12},
	year = 2018
}

@article{guan_fully_2020,
	title = {Fully {Dense} {UNet} for 2-{D} {Sparse} {Photoacoustic} {Tomography} {Artifact} {Removal}},
	volume = {24},
	issn = {2168-2194, 2168-2208},
	doi = {10.1109/JBHI.2019.2912935},
	language = {en},
	number = {2},
	urldate = {2022-02-28},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Guan, Steven and Khan, Amir A. and Sikdar, Siddhartha and Chitnis, Parag V.},
	month = feb,
	year = {2020},
	pages = {568--576}
}

@article{gu_ce-net_2019,
	title = {{CE}-{Net}: {Context} {Encoder} {Network} for {2D} {Medical} {Image} {Segmentation}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	shorttitle = {{CE}-{Net}},
	doi = {10.1109/TMI.2019.2903562},
	language = {en},
	number = {10},
	urldate = {2022-02-28},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Gu, Zaiwang and Cheng, Jun and Fu, Huazhu and Zhou, Kang and Hao, Huaying and Zhao, Yitian and Zhang, Tianyang and Gao, Shenghua and Liu, Jiang},
	month = oct,
	year = {2019},
	pages = {2281-2292}
}

@article{oktay_attention_nodate,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	language = {en},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	pages = {10},
	year = 2018,
	journal = {arXiv}
}

@article{wang_non-local_2020,
	title = {Non-local {U}-{Net} for {Biomedical} {Image} {Segmentation}},
	language = {en},
	urldate = {2021-04-10},
	journal = {arXiv},
	author = {Wang, Zhengyang and Zou, Na and Shen, Dinggang and Ji, Shuiwang},
	month = feb,
	year = {2020},
	note = {arXiv: 1812.04103},
	annote = {Comment: In Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI), 2019},
}

@article{lei_medical_2020,
	title = {Medical {Image} {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Medical {Image} {Segmentation} {Using} {Deep} {Learning}},
	language = {en},
	urldate = {2021-04-20},
	journal = {arXiv},
	author = {Lei, Tao and Wang, Risheng and Wan, Yong and Zhang, Bingtao and Meng, Hongying and Nandi, Asoke K.},
	month = dec,
	year = {2020},
	note = {arXiv: 2009.13120},
}

@article{zhou_unet_2020,
	title = {{UNet}++: {Redesigning} {Skip} {Connections} to {Exploit} {Multiscale} {Features} in {Image} {Segmentation}},
	volume = {39},
	issn = {0278-0062, 1558-254X},
	shorttitle = {{UNet}++},
	url = {https://ieeexplore.ieee.org/document/8932614/},
	doi = {10.1109/TMI.2019.2959609},
	language = {en},
	number = {6},
	urldate = {2021-04-07},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
	month = jun,
	year = {2020},
	pages = {1856--1867},
}

@article{jha_doubleu-net_2020,
	title = {{DoubleU}-{Net}: {A} {Deep} {Convolutional} {Neural} {Network} for {Medical} {Image} {Segmentation}},
	shorttitle = {{DoubleU}-{Net}},
	url = {http://arxiv.org/abs/2006.04868},
	language = {en},
	urldate = {2021-04-21},
	journal = {arXiv:2006.04868 [cs, eess]},
	author = {Jha, Debesh and Riegler, Michael A. and Johansen, Dag and Halvorsen, Pål and Johansen, Håvard D.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04868},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Jha et al. - 2020 - DoubleU-Net A Deep Convolutional Neural Network f.pdf:/Users/jack/Zotero/storage/IHVXVH49/Jha et al. - 2020 - DoubleU-Net A Deep Convolutional Neural Network f.pdf:application/pdf},
}

@inproceedings{long_fully_nodate,
	title = {Fully convolutional networks for semantic segmentation},
	isbn = {1-4673-6964-0},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
	language = {eng},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Image segmentation, Training, Adaptation models, Computer architecture, Convolution, Deconvolution, Semantics},
	pages = {3431--3440},
}


@inproceedings{gao_fully_2018,
	address = {Washington, DC},
	title = {Fully convolutional structured {LSTM} networks for joint {4D} medical image segmentation},
	isbn = {978-1-5386-3636-7},
	url = {https://ieeexplore.ieee.org/document/8363764/},
	doi = {10.1109/ISBI.2018.8363764},
	language = {en},
	urldate = {2022-02-27},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Gao, Yang and Phillips, Jeff M. and Zheng, Yan and Min, Renqiang and Fletcher, P. Thomas and Gerig, Guido},
	month = apr,
	year = {2018},
	pages = {1104--1108},
}

@article{bai_recurrent_2018,
	title = {Recurrent neural networks for aortic image sequence segmentation with sparse annotations},
	url = {http://arxiv.org/abs/1808.00273},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:1808.00273 [cs]},
	author = {Bai, Wenjia and Suzuki, Hideaki and Qin, Chen and Tarroni, Giacomo and Oktay, Ozan and Matthews, Paul M. and Rueckert, Daniel},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.00273},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted for publication by MICCAI 2018},
	file = {Bai et al. - 2018 - Recurrent neural networks for aortic image sequenc.pdf:/Users/jack/Zotero/storage/FK2D27ZA/Bai et al. - 2018 - Recurrent neural networks for aortic image sequenc.pdf:application/pdf},
}

@article{ibtehaz_multiresunet_2020,
	title = {{MultiResUNet} : {Rethinking} the {U}-{Net} {Architecture} for {Multimodal} {Biomedical} {Image} {Segmentation}},
	volume = {121},
	issn = {08936080},
	shorttitle = {{MultiResUNet}},
	url = {http://arxiv.org/abs/1902.04049},
	doi = {10.1016/j.neunet.2019.08.025},
	abstract = {In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. U-Net is the most prominent deep network in this regard, which has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, from extensive experimentations on challenging datasets, we found out that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modiﬁcations to improve upon the already state-of-the-art UNet model. Hence, following the modiﬁcations we develop a novel architecture MultiResUNet as the potential successor to the successful U-Net architecture. We have compared our proposed architecture MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Albeit slight improvements in the cases of ideal images, a remarkable gain in performance has been attained for challenging images. We have evaluated our model on ﬁve diﬀerent datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15\%, 5.07\%, 2.63\%, 1.41\%, and 0.62\% respectively.},
	language = {en},
	urldate = {2022-02-27},
	journal = {Neural Networks},
	author = {Ibtehaz, Nabil and Rahman, M. Sohel},
	month = jan,
	year = {2020},
	note = {arXiv: 1902.04049},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {74--87},
	file = {Ibtehaz and Rahman - 2020 - MultiResUNet  Rethinking the U-Net Architecture f.pdf:/Users/jack/Zotero/storage/4YNB4629/Ibtehaz and Rahman - 2020 - MultiResUNet  Rethinking the U-Net Architecture f.pdf:application/pdf},
}

@article{seo_modified_2020,
	title = {Modified {U}-{Net} ({mU}-{Net}) {With} {Incorporation} of {Object}-{Dependent} {High} {Level} {Features} for {Improved} {Liver} and {Liver}-{Tumor} {Segmentation} in {CT} {Images}},
	volume = {39},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8876857/},
	doi = {10.1109/TMI.2019.2948320},
	language = {en},
	number = {5},
	urldate = {2022-02-27},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Seo, Hyunseok and Huang, Charles and Bassenne, Maxime and Xiao, Ruoxiu and Xing, Lei},
	month = may,
	year = {2020},
	pages = {1316--1325},
	file = {Seo et al. - 2020 - Modified U-Net (mU-Net) With Incorporation of Obje.pdf:/Users/jack/Zotero/storage/FYZBTXHM/Seo et al. - 2020 - Modified U-Net (mU-Net) With Incorporation of Obje.pdf:application/pdf},
}

@article{guibas_synthetic_2018,
	title = {Synthetic {Medical} {Images} from {Dual} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1709.01872},
	language = {en},
	urldate = {2022-02-28},
	journal = {arXiv:1709.01872 [cs]},
	author = {Guibas, John T. and Virdi, Tejpal S. and Li, Peter S.},
	month = jan,
	year = {2018},
	note = {arXiv: 1709.01872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: First two authors contributed equally. Accepted to NIPS 2017 Workshops on Medical Imaging and Machine Learning for Health},
	file = {Guibas et al. - 2018 - Synthetic Medical Images from Dual Generative Adve.pdf:/Users/jack/Zotero/storage/3NG8EQLB/Guibas et al. - 2018 - Synthetic Medical Images from Dual Generative Adve.pdf:application/pdf},
}

@phdthesis{milletary_hough_2017,
	title = {Hough {Voting} {Strategies} for {Segmentation}, {Detection} and {Tracking}},
	school = {Technischen Universität München},
	author = {Milletary, Fausto},
	month = nov,
	year = {2017},
}

@inproceedings{vivanti_automatic_nodate,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Automatic {Liver} {Tumor} {Segmentation} in {Follow}-{Up} {CT} {Scans}: {Preliminary} {Method} and {Results}},
	copyright = {Springer International Publishing Switzerland 2015},
	isbn = {3-319-28193-3},
	abstract = {We present a new, fully automatic algorithm for liver tumors segmentation in follow-up CT studies. The inputs are a baseline CT scan and a delineation of the tumors in it and a follow-up scan; the outputs are the tumors delineations in the follow-up CT scan. The algorithm starts by defining a region of interest using a deformable registration of the baseline scan and tumors delineations to the follow-up CT scan and automatic liver segmentation. Then, it constructs a voxel classifier by training a Convolutional Neural Network (CNN). Finally, it segments the tumor in the follow-up study with the learned classifier. The main novelty of our method is the combination of follow-up based detection with CNN-based segmentation. Our experimental results on 67 tumors from 21 patients with ground-truth segmentations approved by a radiologist yield a success rate of 95.4 \% and an average overlap error of 16.3 \% (std = 10.3).},
	language = {eng},
	booktitle = {Patch-{Based} {Techniques} in {Medical} {Imaging}},
	publisher = {Springer International Publishing},
	author = {Vivanti, Refael and Ephrat, Ariel and Joskowicz, Leo and Lev-Cohain, Naama and Karaaslan, Onur A and Sosna, Jacob},
	year = {2016},
	note = {ISSN: 0302-9743},
	keywords = {Convolutional Neural Network, Deformable Registration, Liver Segmentation, Tumor Delineation, Tumor Segmentation},
	pages = {54--61},
}


@article{menze_multimodal_2015,
	title = {The {Multimodal} {Brain} {Tumor} {Image} {Segmentation} {Benchmark} ({BRATS})},
	volume = {34},
	copyright = {Distributed under a Creative Commons Attribution 4.0 International License},
	issn = {0278-0062},
	abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74\%-85\%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
	language = {eng},
	number = {10},
	journal = {IEEE transactions on medical imaging},
	author = {Menze, Bjoern H and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc-Andre and Arbel, Tal and Avants, Brian B and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J and Criminisi, Antonio and Das, Tilak and Delingette, Herve and Demiralp, Cagatay and Durst, Christopher R and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M and Jena, Raj and John, Nigel M and Konukoglu, Ender and Lashkari, Danial and Mariz, Jose Antonio and Meier, Raphael and Pereira, Sergio and Precup, Doina and Price, Stephen J and Riklin Raviv, Tammy and Reza, Syed M. S and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo-Chang and Shotton, Jamie and Silva, Carlos A and Sousa, Nuno and Subbanna, Nagesh K and Szekely, Gabor and Taylor, Thomas J and Thomas, Owen M and Tustison, Nicholas J and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and Van Leemput, Koen},
	year = {2015},
	note = {Place: PISCATAWAY
Publisher: IEEE},
	keywords = {Algorithms, Benchmark, Benchmark testing, Benchmarking, Benchmarks, Biomedical, Biomedical imaging, Brain, Brain tumors, Cognitive science, Computer Science, Diagnostic imaging, Educational institutions, Electrical \& Electronic, Engineering, Glioma - pathology, Humans, Image segmentation, Imaging Science \& Photographic Technology, Interdisciplinary Applications, Lesions, Life Sciences \& Biomedicine, Magnetic Resonance Imaging - methods, Magnetic Resonance Imaging - standards, MRI, Neuroimaging - methods, Neuroimaging - standards, Nuclear Medicine \& Medical Imaging, Oncology, Oncology/tumor, Radiology, Research, Science \& Technology, Technology, tumor, Usage},
	pages = {1993--2024},
}

@article{li_automatic_2015,
	title = {Automatic {Segmentation} of {Liver} {Tumor} in {CT} {Images} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {3},
	copyright = {COPYRIGHT 2015 Modern Science Publishers},
	issn = {2327-5219},
	abstract = {Liver tumors segmentation from computed tomography (CT) images is an essential task for diagnosis and treatments of liver cancer. However, it is difficult owing to the variability of appearances, fuzzy boundaries, heterogeneous densities, shapes and sizes of lesions. In this paper, an automatic method based on convolutional neural networks (CNNs) is presented to segment lesions from CT images. The CNNs is one of deep learning models with some convolutional filters which can learn hierarchical features from data. We compared the CNNs model to popular machine learning algorithms: AdaBoost, Random Forests (RF), and support vector machine (SVM). These classifiers were trained by handcrafted features containing mean, variance, and contextual features. Experimental evaluation was performed on 30 portal phase enhanced CT images using leave-one-out cross validation. The average Dice Similarity Coefficient (DSC), precision, and recall achieved of 80.06\% [+ or -] 1.63\%, 82.67\% [+ or -] 1.43\%, and 84.34\% [+ or -] 1.61\%, respectively. The results show that the CNNs method has better performance than other methods and is promising in liver tumor segmentation. Keywords Liver Tumor Segmentation, Convolutional Neural Networks, Deep Learning, CT Image},
	language = {eng},
	number = {11},
	journal = {Journal of computer and communications},
	author = {Li, Wen and Jia, Fucang and Hu, Qingmao},
	year = {2015},
	note = {Publisher: Modern Science Publishers},
	keywords = {CT imaging, Data mining, Diagnosis, Image segmentation, Liver cancer, Machine learning, Methods, Neural networks, Research, Usage},
	pages = {146--151},
}

@article{cherukuri_learning_2018,
	title = {Learning {Based} {Segmentation} of {CT} {Brain} {Images}: {Application} to {Postoperative} {Hydrocephalic} {Scans}},
	volume = {65},
	issn = {0018-9294},
	language = {eng},
	number = {8},
	journal = {IEEE transactions on biomedical engineering},
	author = {Cherukuri, Venkateswararao and Ssenyonga, Peter and Warf, Benjamin C and Kulkarni, Abhaya V and Monga, Vishal and Schiff, Steven J},
	year = {2018},
	keywords = {Algorithms, Biomedical, Brain, Brain - diagnostic imaging, Computed tomography, Computer-Assisted - methods, CT image segmentation, Dictionaries, dictionary learning, Engineering, Hospitals, Humans, hydrocephalus, Hydrocephalus - diagnostic imaging, Image Interpretation, Image segmentation, Infant, Machine Learning, neurosurgery, Science \& Technology, subdural hematoma, Technology, Tomography, Training, volume, X-Ray Computed - methods},
	pages = {1871--1884},
}

@article{cheng_superpixel_2013,
	title = {Superpixel {Classification} {Based} {Optic} {Disc} and {Optic} {Cup} {Segmentation} for {Glaucoma} {Screening}},
	volume = {32},
	issn = {0278-0062},
	abstract = {Glaucoma is a chronic eye disease that leads to vision loss. As it cannot be cured, detecting the disease in time is important. Current tests using intraocular pressure (IOP) are not sensitive enough for population based glaucoma screening. Optic nerve head assessment in retinal fundus images is both more promising and superior. This paper proposes optic disc and optic cup segmentation using superpixel classification for glaucoma screening. In optic disc segmentation, histograms, and center surround statistics are used to classify each superpixel as disc or non-disc. A self-assessment reliability score is computed to evaluate the quality of the automated optic disc segmentation. For optic cup segmentation, in addition to the histograms and center surround statistics, the location information is also included into the feature space to boost the performance. The proposed segmentation methods have been evaluated in a database of 650 images with optic disc and optic cup boundaries manually marked by trained professionals. Experimental results show an average overlapping error of 9.5\% and 24.1\% in optic disc and optic cup segmentation, respectively. The results also show an increase in overlapping error as the reliability score is reduced, which justifies the effectiveness of the self-assessment. The segmented optic disc and optic cup are then used to compute the cup to disc ratio for glaucoma screening. Our proposed method achieves areas under curve of 0.800 and 0.822 in two data sets, which is higher than other methods. The methods can be used for segmentation and glaucoma screening. The self-assessment will be used as an indicator of cases with large errors and enhance the clinical deployment of the automatic segmentation and screening.},
	language = {eng},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Cheng, Jun and Liu, Jiang and Xu, Yanwu and Yin, Fengshou and Wong, D. W. K and Tan, Ngan-Meng and Tao, Dacheng and Cheng, Ching-Yu and Aung, Tin and Wong, Tien Yin},
	year = {2013},
	keywords = {Adaptive optics, Area Under Curve, Biomedical, Computer Science, Computer-Assisted - methods, Databases, Deformable models, Diagnosis, Diagnostic imaging, Diagnostic Techniques, Electrical \& Electronic, Engineering, Factual, Glaucoma, Glaucoma - diagnosis, Glaucoma - pathology, Glaucoma screening, Histograms, Humans, Image color analysis, Image Interpretation, Image segmentation, Imaging Science \& Photographic Technology, Interdisciplinary Applications, Life Sciences \& Biomedicine, Methods, Nuclear Medicine \& Medical Imaging, Observations, Ophthalmological, optic cup segmentation, Optic disc, optic disc segmentation, Optic Disk - anatomy \& histology, Optical imaging, Optical sensors, Radiology, Reproducibility of Results, Research, Science \& Technology, Support Vector Machine, Technology},
	pages = {1019--1032},
}

@article{fu_joint_2018,
	title = {Joint {Optic} {Disc} and {Cup} {Segmentation} {Based} on {Multi}-{Label} {Deep} {Network} and {Polar} {Transformation}},
	volume = {37},
	copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
	issn = {0278-0062},
	language = {eng},
	number = {7},
	journal = {IEEE transactions on medical imaging},
	author = {Fu, Huazhu and Cheng, Jun and Xu, Yanwu and Wong, Damon Wing Kee and Liu, Jiang and Cao, Xiaochun},
	year = {2018},
	keywords = {Biomedical, Biomedical optical imaging, Computer Science, Computer Science - Computer Vision and Pattern Recognition, Computer-Assisted - methods, cup to disc ratio, Databases, Deep Learning, Diagnostic Techniques, Electrical \& Electronic, Engineering, Factual, Glaucoma - diagnostic imaging, glaucoma screening, Humans, Image Interpretation, Image segmentation, Imaging Science \& Photographic Technology, Interdisciplinary Applications, Life Sciences \& Biomedicine, Machine learning, Nuclear Medicine \& Medical Imaging, Ophthalmological, optic cup segmentation, optic disc segmentation, Optic Disk - diagnostic imaging, Optical imaging, Optical losses, Radiology, Science \& Technology, Technology, Visualization},
	pages = {1597--1605},
}

@article{song_dual-channel_2017,
	title = {Dual-{Channel} {Active} {Contour} {Model} for {Megakaryocytic} {Cell} {Segmentation} in {Bone} {Marrow} {Trephine} {Histology} {Images}},
	volume = {64},
	issn = {0018-9294},
	language = {eng},
	number = {12},
	journal = {IEEE transactions on biomedical engineering},
	author = {Song, Tzu-Hsi and Sanchez, Victor and EIDaly, Hesham and Rajpoot, Nasir M},
	year = {2017},
	keywords = {Active contours, Artificial intelligence, Biological system modeling, Biomedical, Biomedical imaging, bone marrow trephine biopsies, Bones, digital pathology, Engineering, Image color analysis, Image segmentation, Level set, megakaryocyte (MK) segmentation, Research, Science \& Technology, Technology},
	pages = {2913--2923},
}

@article{wang_central_2017,
	title = {Central focused convolutional neural networks: {Developing} a data-driven model for lung nodule segmentation},
	volume = {40},
	copyright = {2017 The Authors},
	issn = {1361-8415},
	abstract = {•A data-driven lung nodule segmentation method without involving shape hypothesis.•Two-branch convolutional neural networks extract both 3D and multi-scale 2D features.•A novel central pooling layer is proposed for feature selection.•We propose a weighted sampling method to solve imbalanced training label problem.•The method shows strong performance for segmenting juxtapleural nodules. [Display omitted] Accurate lung nodule segmentation from computed tomography (CT) images is of great importance for image-driven lung cancer analysis. However, the heterogeneity of lung nodules and the presence of similar visual characteristics between nodules and their surroundings make it difficult for robust nodule segmentation. In this study, we propose a data-driven model, termed the Central Focused Convolutional Neural Networks (CF-CNN), to segment lung nodules from heterogeneous CT images. Our approach combines two key insights: 1) the proposed model captures a diverse set of nodule-sensitive features from both 3-D and 2-D CT images simultaneously; 2) when classifying an image voxel, the effects of its neighbor voxels can vary according to their spatial locations. We describe this phenomenon by proposing a novel central pooling layer retaining much information on voxel patch center, followed by a multi-scale patch learning strategy. Moreover, we design a weighted sampling to facilitate the model training, where training samples are selected according to their degree of segmentation difficulty. The proposed method has been extensively evaluated on the public LIDC dataset including 893 nodules and an independent dataset with 74 nodules from Guangdong General Hospital (GDGH). We showed that CF-CNN achieved superior segmentation performance with average dice scores of 82.15\% and 80.02\% for the two datasets respectively. Moreover, we compared our results with the inter-radiologists consistency on LIDC dataset, showing a difference in average dice score of only 1.98\%.},
	language = {eng},
	journal = {Medical image analysis},
	author = {Wang, Shuo and Zhou, Mu and Liu, Zaiyi and Liu, Zhenyu and Gu, Dongsheng and Zang, Yali and Dong, Di and Gevaert, Olivier and Tian, Jie},
	year = {2017},
	keywords = {Artificial Intelligence, Biomedical, Computer Science, Computer-aided diagnosis, Computer-Assisted - methods, Convolutional neural networks, CT imaging, Deep learning, Diagnosis, Engineering, Humans, Interdisciplinary Applications, Life Sciences \& Biomedicine, Lung cancer, Lung Neoplasms - diagnostic imaging, Lung nodule segmentation, Machine Learning, Neural networks, Neural Networks (Computer), Nuclear Medicine \& Medical Imaging, Radiology, Science \& Technology, Sensitivity and Specificity, Technology, Tomography, X-Ray Computed - methods},
	pages = {172--183},
}

@article{onishi_multiplanar_2019,
	title = {Multiplanar analysis for pulmonary nodule classification in {CT} images using deep convolutional neural network and generative adversarial networks},
	volume = {15},
	copyright = {CARS 2019},
	issn = {1861-6410},
	abstract = {Purpose Early detection and treatment of lung cancer holds great importance. However, pulmonary-nodule classification using CT images alone is difficult to realize. To address this concern, a method for pulmonary-nodule classification based on a deep convolutional neural network (DCNN) and generative adversarial networks (GAN) has previously been proposed by the authors. In that method, the said classification was performed exclusively using axial cross sections of pulmonary nodules. During actual medical-examination procedures, however, a comprehensive judgment can only be made via observation of various pulmonary-nodule cross sections. In the present study, a comprehensive analysis was performed by extending the application of the previously proposed DCNN- and GAN-based automatic classification method to multiple cross sections of pulmonary nodules. Methods Using the proposed method, CT images of 60 cases with confirmed pathological diagnosis by biopsy are analyzed. Firstly, multiplanar images of the pulmonary nodule are generated. Classification training was performed for three DCNNs. A certain pretraining was initially performed using GAN-generated nodule images. This was followed by fine-tuning of each pretrained DCNN using original nodule images provided as input. Results As a result of the evaluation, the specificity was 77.8\% and the sensitivity was 93.9\%. Additionally, the specificity was observed to have improved by 11.1\% without any reduction in the sensitivity, compared to our previous report. Conclusion This study reports development of a comprehensive analysis method to classify pulmonary nodules at multiple sections using GAN and DCNN. The effectiveness of the proposed discrimination method based on use of multiplanar images has been demonstrated to be improved compared to that realized in a previous study reported by the authors. In addition, the possibility of enhancing classification accuracy via application of GAN-generated images, instead of data augmentation, for pretraining even for medical datasets that contain relatively few images has also been demonstrated.},
	language = {eng},
	number = {1},
	journal = {International journal for computer assisted radiology and surgery},
	author = {Onishi, Yuya and Teramoto, Atsushi and Tsujimoto, Masakazu and Tsukamoto, Tetsuya and Saito, Kuniaki and Toyama, Hiroshi and Imaizumi, Kazuyoshi and Fujita, Hiroshi},
	year = {2019},
	publisher = {Springer International Publishing},
	pages = {173--178}
}

@article{chen_rethinking_2017,
	title = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
	copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
	abstract = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark.},
	language = {eng},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {\_eprint: https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
	pages = {1735--1780},
}

@article{chen_feature_2019,
	title = {Feature {Fusion} {Encoder} {Decoder} {Network} {For} {Automatic} {Liver} {Lesion} {Segmentation}},
	copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
	abstract = {Liver lesion segmentation is a difficult yet critical task for medical image analysis. Recently, deep learning based image segmentation methods have achieved promising performance, which can be divided into three categories: 2D, 2.5D and 3D, based on the dimensionality of the models. However, 2.5D and 3D methods can have very high complexity and 2D methods may not perform satisfactorily. To obtain competitive performance with low complexity, in this paper, we propose a Feature-fusion Encoder-Decoder Network (FED-Net) based 2D segmentation model to tackle the challenging problem of liver lesion segmentation from CT images. Our feature fusion method is based on the attention mechanism, which fuses high-level features carrying semantic information with low-level features having image details. Additionally, to compensate for the information loss during the upsampling process, a dense upsampling convolution and a residual convolutional structure are proposed. We tested our method on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge and achieved competitive results compared with other state-of-the-art methods.},
	language = {eng},
	author = {Chen, Xueying and Zhang, Rong and Yan, Pingkun},
	year = {2019},
}

@article{christ_automatic_2016,
	title = {Automatic {Liver} and {Lesion} {Segmentation} in {CT} {Using} {Cascaded} {Fully} {Convolutional} {Neural} {Networks} and {3D} {Conditional} {Random} {Fields}},
	volume = {9901},
	url = {http://arxiv.org/abs/1610.02177},
	doi = {10.1007/978-3-319-46723-8_48},
	language = {en},
	urldate = {2022-03-01},
	journal = {arXiv:1610.02177 [cs]},
	author = {Christ, Patrick Ferdinand and Elshaer, Mohamed Ezzeldin A. and Ettlinger, Florian and Tatavarty, Sunil and Bickel, Marc and Bilic, Patrick and Rempfler, Markus and Armbruster, Marco and Hofmann, Felix and D'Anastasi, Melvin and Sommer, Wieland H. and Ahmadi, Seyed-Ahmad and Menze, Bjoern H.},
	year = {2016},
	note = {arXiv: 1610.02177},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {415--423},
	annote = {Comment: Accepted at MICCAI 2016. Source code available on https://github.com/IBBM/Cascaded-FCN},
}

@article{kaluva_2d-densely_2018,
	title = {{2D}-{Densely} {Connected} {Convolution} {Neural} {Networks} for automatic {Liver} and {Tumor} {Segmentation}},
	url = {http://arxiv.org/abs/1802.02182},
	abstract = {In this paper we propose a fully automatic 2-stage cascaded approach for segmentation of liver and its tumors in CT (Computed Tomography) images using densely connected fully convolutional neural network (DenseNet). We independently train liver and tumor segmentation models and cascade them for a combined segmentation of the liver and its tumor. The ﬁrst stage involves segmentation of liver and the second stage uses the ﬁrst stage’s segmentation results for localization of liver and henceforth tumor segmentations inside liver region. The liver model was trained on the down-sampled axial slices (256 × 256), whereas for the tumor model no down-sampling of slices was done, but instead it was trained on the CT axial slices windowed at three different Hounsﬁeld (HU) levels. On the test set our model achieved a global dice score of 0.923 and 0.625 on liver and tumor respectively. The computed tumor burden had an rmse of 0.044.},
	language = {en},
	urldate = {2022-03-01},
	journal = {arXiv:1802.02182 [cs]},
	author = {Kaluva, Krishna Chaitanya and Khened, Mahendra and Kori, Avinash and Krishnamurthi, Ganapathy},
	month = jan,
	year = {2018},
	note = {arXiv: 1802.02182},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Kaluva et al. - 2018 - 2D-Densely Connected Convolution Neural Networks f.pdf:/Users/jack/Zotero/storage/UEJ8KBGS/Kaluva et al. - 2018 - 2D-Densely Connected Convolution Neural Networks f.pdf:application/pdf},
}

@inproceedings{tang_dsl_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DSL}: {Automatic} {Liver} {Segmentation} with {Faster} {R}-{CNN} and {DeepLab}},
	copyright = {Springer Nature Switzerland AG 2018},
	isbn = {3-030-01420-7},
	language = {eng},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2018},
	publisher = {Springer International Publishing},
	author = {Tang, Wei and Zou, Dongsheng and Yang, Su and Shi, Jing},
	year = {2018},
	keywords = {DeepLab, Detection, Faster R-CNN, Segmentation},
	pages = {137--147},
}

@incollection{feng_automatic_2018,
	address = {Singapore},
	series = {Lecture {Notes} in {Electrical} {Engineering}},
	title = {Automatic {Liver} and {Tumor} {Segmentation} of {CT} {Based} on {Cascaded} {U}-{Net}},
	copyright = {Springer Nature Singapore Pte Ltd. 2019},
	isbn = {9789811322907},
	language = {eng},
	booktitle = {Proceedings of 2018 {Chinese} {Intelligent} {Systems} {Conference}},
	publisher = {Springer Singapore},
	author = {Feng, Xiaorui and Wang, Chaoli and Cheng, Shuqun and Guo, Lei},
	year = {2018},
	pages = {155--164},
}

@inproceedings{mahapatra_efficient_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {Active} {Learning} for {Image} {Classification} and {Segmentation} {Using} a {Sample} {Selection} and {Conditional} {Generative} {Adversarial} {Network}},
	copyright = {Springer Nature Switzerland AG 2018},
	isbn = {978-3-030-00933-5},
	language = {eng},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Mahapatra, Dwarikanath and Bozorgtabar, Behzad and Thiran, Jean-Philippe and Reyes, Mauricio},
	year = {2018},
	pages = {580--588},
}

@article{kucewicz_functional_2007,
	title = {Functional {Tissue} {Pulsatility} {Imaging} of the {Brain} {During} {Visual} {Stimulation}},
	volume = {33},
	issn = {03015629},
	doi = {10.1016/j.ultrasmedbio.2006.11.008},
	language = {en},
	number = {5},
	urldate = {2022-02-23},
	journal = {Ultrasound in Medicine \& Biology},
	author = {Kucewicz, John C. and Dunmire, Barbrina and Leotta, Daniel F. and Panagiotides, Heracles and Paun, Marla and Beach, Kirk W.},
	month = may,
	year = {2007},
	pages = {681--690},
}

@incollection{evans_chapter_2000,
	edition = {2nd},
	title = {Chapter 11, {Signal} {Processing} for {Colour} {Flow} {Imaging}},
	booktitle = {Doppler {Ultrasound}: {Physics}, {Instrumentation}, and {Signal} {Processing}},
	publisher = {Chichester, England: John Wiley and Sons, Ltd},
	author = {Evans, David and McDicken, W. Norman},
	year = {2000},
	pages = {245--287},
}
