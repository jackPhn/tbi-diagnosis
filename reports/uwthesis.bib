 
Bibtex (bib) file for the thesis style document.
 
@article{explosive,
	title = "Explosive blast neurotrauma",
	journal = "J Neurotrauma",
	author = "G, Ling and F, Bandak and R, Armonda and G, Grant and J, Ecklund",
   	doi = "10.1089/neu.2007.0484",
	month = jun,
	year = 2009} 

@misc{TBI,
	title = {Traumatic brain injury},
	howpublished  = {\url{https://www.hopkinsmedicine.org/health/conditions-and-diseases/traumatic-brain-injury}},
	note = "Accessed: 2022-02-20",
}

@article{hyder_impact_2007,
	title = {The impact of traumatic brain injuries: A global perspective},
	volume = {22},
	issn = {10538135, 18786448},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/NRE-2007-22502},
	doi = {10.3233/NRE-2007-22502},
	shorttitle = {The impact of traumatic brain injuries},
	pages = {341--353},
	number = {5},
	journaltitle = {{NeuroRehabilitation}},
	shortjournal = {{NRE}},
	author = {Hyder, Adnan A. and Wunderlich, Colleen A. and Puvanachandra, Prasanthi and Gururaj, G. and Kobusingye, Olive C.},
	editor = {Neufeld, Jacob A.},
	urldate = {2022-02-21},
	date = {2007-12-07},
	langid = {english},
}

@article{heit_imaging_2017,
	title = {Imaging of Intracranial Hemorrhage},
	volume = {19},
	issn = {2287-6391, 2287-6405},
	url = {http://j-stroke.org/journal/view.php?doi=10.5853/jos.2016.00563},
	doi = {10.5853/jos.2016.00563},
	pages = {11--27},
	number = {1},
	journaltitle = {Journal of Stroke},
	shortjournal = {J Stroke},
	author = {Heit, Jeremy J. and Iv, Michael and Wintermark, Max},
	urldate = {2021-04-07},
	date = {2017-01-31},
	langid = {english},
}

@misc{cranial_ult,
   	title = {Cranial Ultrasound},
	howpublished  = {\url{https://www.healthlinkbc.ca/tests-treatments-medications/medical-tests/cranial-ultrasound}},
	note = "Accessed: 2022-02-21"
}

@misc{trauma_care,
	title = {Trauma Care},
	howpublished = {\url{https://www.uwmedicine.org/specialties/emergency-medicine/trauma-care}},
	note = "Accessed: 2022-02-22"
}

@book{management_2000,
	location = {United States},
	title = {Management and prognosis of severe traumatic brain injury},
	isbn = {978-0-9703144-0-6},
	publisher = {Brain Trauma Foundation},
	year = {2000},
	langid = {english},
}

@article{kucewicz_tissue_2008,
	title = {Tissue Pulsatility Imaging of Cerebral Vasoreactivity During Hyperventilation},
	volume = {34},
	issn = {03015629},
	doi = {10.1016/j.ultrasmedbio.2008.01.001},
	abstract = {Tissue pulsatility imaging ({TPI}) is an ultrasonic technique that is being developed at the University of Washington to measure tissue displacement or strain as a result of blood ﬂow over the cardiac and respiratory cycles. This technique is based in principle on plethysmography, an older nonultrasound technology for measuring expansion of a whole limb or body part due to perfusion. {TPI} adapts tissue Doppler signal processing methods to measure the “plethysmographic” signal from hundreds or thousands of sample volumes in an ultrasound image plane. This paper presents a feasibility study to determine if {TPI} can be used to assess cerebral vasoreactivity. Ultrasound data were collected transcranially through the temporal acoustic window from four subjects before, during and after voluntary hyperventilation. In each subject, decreases in tissue pulsatility during hyperventilation were observed that were statistically correlated with the subject’s end-tidal {CO}2 measurements. (E-mail: kucewicz@u.washington.edu) © 2008 World Federation for Ultrasound in Medicine \& Biology.},
	pages = {1200--1208},
	number = {8},
	journaltitle = {Ultrasound in Medicine \& Biology},
	shortjournal = {Ultrasound in Medicine \& Biology},
	author = {Kucewicz, John C. and Dunmire, Barbrina and Giardino, Nicholas D. and Leotta, Daniel F. and Paun, Marla and Dager, Stephen R. and Beach, Kirk W.},
	year = {2008},
	langid = {english},
}

@book{hemodynamics,
   author = {Strandness and Summer},
   title = "Hemodynamics for Surgeons",
   publisher = "New York: Grune and Strattion",
   year = 1975}

@article{pulsatile_echo,
  author = {J Campbell and J Clark and D White and C Jenkins},
  title = "Pulsatile echo-encephalography",
  journal = "Acta Neurologica Scandinavica. Supplementum",
  volume = 45,
  year = 1970,
  pages = "1-57"}

@article{sandler_mobilenetv2_2019,
	title = {{MobileNetV2}: {Inverted} {Residuals} and {Linear} {Bottlenecks}},
	shorttitle = {{MobileNetV2}},
	url = {http://arxiv.org/abs/1801.04381},
	language = {en},
	urldate = {2022-02-24},
	journal = {arXiv:1801.04381 [cs]},
	author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
	month = mar,
	year = {2019},
	note = {arXiv: 1801.04381},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{isola_image_image_2018,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	language = {en},
	urldate = {2022-02-24},
	journal = {arXiv:1611.07004 [cs]},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv: 1611.07004},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
}

@article{zhang_resnest_2020,
	title = {{ResNeSt}: {Split}-{Attention} {Networks}},
	shorttitle = {{ResNeSt}},
	url = {http://arxiv.org/abs/2004.08955},
	language = {en},
	urldate = {2022-02-24},
	journal = {arXiv:2004.08955 [cs]},
	author = {Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.08955},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{navab_u-net_2015,
	address = {Cham},
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	volume = {9351},
	isbn = {978-3-319-24573-7 978-3-319-24574-4},
	shorttitle = {U-{Net}},
	language = {en},
	urldate = {2021-04-07},
	booktitle = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
	publisher = {Springer International Publishing},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	year = {2015},
	doi = {10.1007/978-3-319-24574-4_28},
	pages = {234--241},
}

@article{cicek_3d_2016,
	title = {{3D} {U}-{Net}: {Learning} {Dense} {Volumetric} {Segmentation} from {Sparse} {Annotation}},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:1606.06650 [cs]},
	author = {Çiçek, Özgün and Abdulkadir, Ahmed and Lienkamp, Soeren S. and Brox, Thomas and Ronneberger, Olaf},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.06650},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Conditionally accepted for MICCAI 2016},
}

@article{milletari_v-net_2016,
	title = {V-{Net}: {Fully} {Convolutional} {Neural} {Networks} for {Volumetric} {Medical} {Image} {Segmentation}},
	shorttitle = {V-{Net}},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:1606.04797 [cs]},
	author = {Milletari, Fausto and Navab, Nassir and Ahmadi, Seyed-Ahmad},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.04797},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
}


@article{alom_recurrent_nodate,
	title = {Recurrent {Residual} {Convolutional} {Neural} {Network} based on {U}-{Net} ({R2U}-{Net}) for {Medical} {Image} {Segmentation}},
	language = {en},
	journal = {arXiv},
	author = {Alom, Zahangir and Taha, Tarek M and Asari, Vijayan K},
	pages = {12},
	year = 2018
}


@article{guan_fully_2020,
	title = {Fully {Dense} {UNet} for 2-{D} {Sparse} {Photoacoustic} {Tomography} {Artifact} {Removal}},
	volume = {24},
	issn = {2168-2194, 2168-2208},
	doi = {10.1109/JBHI.2019.2912935},
	language = {en},
	number = {2},
	urldate = {2022-02-28},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Guan, Steven and Khan, Amir A. and Sikdar, Siddhartha and Chitnis, Parag V.},
	month = feb,
	year = {2020},
	pages = {568--576}
}

@article{gu_ce-net_2019,
	title = {{CE}-{Net}: {Context} {Encoder} {Network} for {2D} {Medical} {Image} {Segmentation}},
	volume = {38},
	issn = {0278-0062, 1558-254X},
	shorttitle = {{CE}-{Net}},
	doi = {10.1109/TMI.2019.2903562},
	language = {en},
	number = {10},
	urldate = {2022-02-28},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Gu, Zaiwang and Cheng, Jun and Fu, Huazhu and Zhou, Kang and Hao, Huaying and Zhao, Yitian and Zhang, Tianyang and Gao, Shenghua and Liu, Jiang},
	month = oct,
	year = {2019},
	pages = {2281-2292}
}

@article{oktay_attention_nodate,
	title = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
	language = {en},
	author = {Oktay, Ozan and Schlemper, Jo and Folgoc, Loic Le and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Hammerla, Nils Y and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
	pages = {10},
	year = 2018,
	journal = {arXiv}
}

@article{wang_non-local_2020,
	title = {Non-local {U}-{Net} for {Biomedical} {Image} {Segmentation}},
	language = {en},
	urldate = {2021-04-10},
	journal = {arXiv},
	author = {Wang, Zhengyang and Zou, Na and Shen, Dinggang and Ji, Shuiwang},
	month = feb,
	year = {2020},
	note = {arXiv: 1812.04103},
	annote = {Comment: In Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI), 2019},
}

@article{lei_medical_2020,
	title = {Medical {Image} {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	shorttitle = {Medical {Image} {Segmentation} {Using} {Deep} {Learning}},
	language = {en},
	urldate = {2021-04-20},
	journal = {arXiv},
	author = {Lei, Tao and Wang, Risheng and Wan, Yong and Zhang, Bingtao and Meng, Hongying and Nandi, Asoke K.},
	month = dec,
	year = {2020},
	note = {arXiv: 2009.13120},
}

@article{zhou_unet_2020,
	title = {{UNet}++: {Redesigning} {Skip} {Connections} to {Exploit} {Multiscale} {Features} in {Image} {Segmentation}},
	volume = {39},
	issn = {0278-0062, 1558-254X},
	shorttitle = {{UNet}++},
	url = {https://ieeexplore.ieee.org/document/8932614/},
	doi = {10.1109/TMI.2019.2959609},
	language = {en},
	number = {6},
	urldate = {2021-04-07},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
	month = jun,
	year = {2020},
	pages = {1856--1867},
}

@article{jha_doubleu-net_2020,
	title = {{DoubleU}-{Net}: {A} {Deep} {Convolutional} {Neural} {Network} for {Medical} {Image} {Segmentation}},
	shorttitle = {{DoubleU}-{Net}},
	url = {http://arxiv.org/abs/2006.04868},
	language = {en},
	urldate = {2021-04-21},
	journal = {arXiv:2006.04868 [cs, eess]},
	author = {Jha, Debesh and Riegler, Michael A. and Johansen, Dag and Halvorsen, Pål and Johansen, Håvard D.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04868},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {Jha et al. - 2020 - DoubleU-Net A Deep Convolutional Neural Network f.pdf:/Users/jack/Zotero/storage/IHVXVH49/Jha et al. - 2020 - DoubleU-Net A Deep Convolutional Neural Network f.pdf:application/pdf},
}

@article{long_fully_nodate,
	title = {Fully {Convolutional} {Networks} for {Semantic} {Segmentation}},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [4] to the segmentation task. We then deﬁne a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
	language = {en},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	pages = {10},
	file = {Long et al. - Fully Convolutional Networks for Semantic Segmenta.pdf:/Users/jack/Zotero/storage/9FMGSXZF/Long et al. - Fully Convolutional Networks for Semantic Segmenta.pdf:application/pdf},
}

@inproceedings{gao_fully_2018,
	address = {Washington, DC},
	title = {Fully convolutional structured {LSTM} networks for joint {4D} medical image segmentation},
	isbn = {978-1-5386-3636-7},
	url = {https://ieeexplore.ieee.org/document/8363764/},
	doi = {10.1109/ISBI.2018.8363764},
	language = {en},
	urldate = {2022-02-27},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	publisher = {IEEE},
	author = {Gao, Yang and Phillips, Jeff M. and Zheng, Yan and Min, Renqiang and Fletcher, P. Thomas and Gerig, Guido},
	month = apr,
	year = {2018},
	pages = {1104--1108},
}

@article{bai_recurrent_2018,
	title = {Recurrent neural networks for aortic image sequence segmentation with sparse annotations},
	url = {http://arxiv.org/abs/1808.00273},
	language = {en},
	urldate = {2022-02-27},
	journal = {arXiv:1808.00273 [cs]},
	author = {Bai, Wenjia and Suzuki, Hideaki and Qin, Chen and Tarroni, Giacomo and Oktay, Ozan and Matthews, Paul M. and Rueckert, Daniel},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.00273},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted for publication by MICCAI 2018},
	file = {Bai et al. - 2018 - Recurrent neural networks for aortic image sequenc.pdf:/Users/jack/Zotero/storage/FK2D27ZA/Bai et al. - 2018 - Recurrent neural networks for aortic image sequenc.pdf:application/pdf},
}

@article{ibtehaz_multiresunet_2020,
	title = {{MultiResUNet} : {Rethinking} the {U}-{Net} {Architecture} for {Multimodal} {Biomedical} {Image} {Segmentation}},
	volume = {121},
	issn = {08936080},
	shorttitle = {{MultiResUNet}},
	url = {http://arxiv.org/abs/1902.04049},
	doi = {10.1016/j.neunet.2019.08.025},
	abstract = {In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation. U-Net is the most prominent deep network in this regard, which has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, from extensive experimentations on challenging datasets, we found out that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modiﬁcations to improve upon the already state-of-the-art UNet model. Hence, following the modiﬁcations we develop a novel architecture MultiResUNet as the potential successor to the successful U-Net architecture. We have compared our proposed architecture MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Albeit slight improvements in the cases of ideal images, a remarkable gain in performance has been attained for challenging images. We have evaluated our model on ﬁve diﬀerent datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15\%, 5.07\%, 2.63\%, 1.41\%, and 0.62\% respectively.},
	language = {en},
	urldate = {2022-02-27},
	journal = {Neural Networks},
	author = {Ibtehaz, Nabil and Rahman, M. Sohel},
	month = jan,
	year = {2020},
	note = {arXiv: 1902.04049},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {74--87},
	file = {Ibtehaz and Rahman - 2020 - MultiResUNet  Rethinking the U-Net Architecture f.pdf:/Users/jack/Zotero/storage/4YNB4629/Ibtehaz and Rahman - 2020 - MultiResUNet  Rethinking the U-Net Architecture f.pdf:application/pdf},
}

@article{seo_modified_2020,
	title = {Modified {U}-{Net} ({mU}-{Net}) {With} {Incorporation} of {Object}-{Dependent} {High} {Level} {Features} for {Improved} {Liver} and {Liver}-{Tumor} {Segmentation} in {CT} {Images}},
	volume = {39},
	issn = {0278-0062, 1558-254X},
	url = {https://ieeexplore.ieee.org/document/8876857/},
	doi = {10.1109/TMI.2019.2948320},
	language = {en},
	number = {5},
	urldate = {2022-02-27},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Seo, Hyunseok and Huang, Charles and Bassenne, Maxime and Xiao, Ruoxiu and Xing, Lei},
	month = may,
	year = {2020},
	pages = {1316--1325},
	file = {Seo et al. - 2020 - Modified U-Net (mU-Net) With Incorporation of Obje.pdf:/Users/jack/Zotero/storage/FYZBTXHM/Seo et al. - 2020 - Modified U-Net (mU-Net) With Incorporation of Obje.pdf:application/pdf},
}

@article{guibas_synthetic_2018,
	title = {Synthetic {Medical} {Images} from {Dual} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1709.01872},
	language = {en},
	urldate = {2022-02-28},
	journal = {arXiv:1709.01872 [cs]},
	author = {Guibas, John T. and Virdi, Tejpal S. and Li, Peter S.},
	month = jan,
	year = {2018},
	note = {arXiv: 1709.01872},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: First two authors contributed equally. Accepted to NIPS 2017 Workshops on Medical Imaging and Machine Learning for Health},
	file = {Guibas et al. - 2018 - Synthetic Medical Images from Dual Generative Adve.pdf:/Users/jack/Zotero/storage/3NG8EQLB/Guibas et al. - 2018 - Synthetic Medical Images from Dual Generative Adve.pdf:application/pdf},
}

@phdthesis{milletary_hough_2017,
	title = {Hough {Voting} {Strategies} for {Segmentation}, {Detection} and {Tracking}},
	school = {Technischen Universität München},
	author = {Milletary, Fausto},
	month = nov,
	year = {2017},
}

@article{vivanti_automatic_nodate,
	title = {Automatic liver tumor segmentation in follow-up {CT} studies using {Convolutional} {Neural} {Networks}},
	abstract = {We present a new, fully automatic algorithm for liver tumors segmentation in follow-up CT studies. The inputs are a baseline CT scan and a delineation of the tumors in it and a follow-up scan; the outputs are the tumors delineations in the follow-up CT scan. The algorithm consists of four steps: 1) deformable registration of the baseline scan and tumors delineations to the followup CT scan; 2) automatic segmentation of the liver; 3) training a Convolutional Neural Network (CNN) as a voxel classifier on all baseline; 4) segmentation of the tumor in the follow-up study with the learned classifier. The main novelty of our method is the combination of follow-up based detection with CNN-based segmentation. Our experimental results on 67 tumors from 21 patients with ground-truth segmentations approved by a radiologist yield an average overlap error of 16.26\% (std=10.33).},
	language = {en},
	author = {Vivanti, R and Ephrat, A and Joskowicz, L and Lev-Cohain, N and Sosna, J},
	pages = {9},
}

@article{menze_multimodal_2015,
	title = {The {Multimodal} {Brain} {Tumor} {Image} {Segmentation} {Benchmark} ({BRATS})},
	volume = {34},
	copyright = {Distributed under a Creative Commons Attribution 4.0 International License},
	issn = {0278-0062},
	abstract = {In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74\%-85\%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.},
	language = {eng},
	number = {10},
	journal = {IEEE transactions on medical imaging},
	author = {Menze, Bjoern H and Jakab, Andras and Bauer, Stefan and Kalpathy-Cramer, Jayashree and Farahani, Keyvan and Kirby, Justin and Burren, Yuliya and Porz, Nicole and Slotboom, Johannes and Wiest, Roland and Lanczi, Levente and Gerstner, Elizabeth and Weber, Marc-Andre and Arbel, Tal and Avants, Brian B and Ayache, Nicholas and Buendia, Patricia and Collins, D. Louis and Cordier, Nicolas and Corso, Jason J and Criminisi, Antonio and Das, Tilak and Delingette, Herve and Demiralp, Cagatay and Durst, Christopher R and Dojat, Michel and Doyle, Senan and Festa, Joana and Forbes, Florence and Geremia, Ezequiel and Glocker, Ben and Golland, Polina and Guo, Xiaotao and Hamamci, Andac and Iftekharuddin, Khan M and Jena, Raj and John, Nigel M and Konukoglu, Ender and Lashkari, Danial and Mariz, Jose Antonio and Meier, Raphael and Pereira, Sergio and Precup, Doina and Price, Stephen J and Riklin Raviv, Tammy and Reza, Syed M. S and Ryan, Michael and Sarikaya, Duygu and Schwartz, Lawrence and Shin, Hoo-Chang and Shotton, Jamie and Silva, Carlos A and Sousa, Nuno and Subbanna, Nagesh K and Szekely, Gabor and Taylor, Thomas J and Thomas, Owen M and Tustison, Nicholas J and Unal, Gozde and Vasseur, Flor and Wintermark, Max and Ye, Dong Hye and Zhao, Liang and Zhao, Binsheng and Zikic, Darko and Prastawa, Marcel and Reyes, Mauricio and Van Leemput, Koen},
	year = {2015},
	note = {Place: PISCATAWAY
Publisher: IEEE},
	keywords = {Algorithms, Benchmark, Benchmark testing, Benchmarking, Benchmarks, Biomedical, Biomedical imaging, Brain, Brain tumors, Cognitive science, Computer Science, Diagnostic imaging, Educational institutions, Electrical \& Electronic, Engineering, Glioma - pathology, Humans, Image segmentation, Imaging Science \& Photographic Technology, Interdisciplinary Applications, Lesions, Life Sciences \& Biomedicine, Magnetic Resonance Imaging - methods, Magnetic Resonance Imaging - standards, MRI, Neuroimaging - methods, Neuroimaging - standards, Nuclear Medicine \& Medical Imaging, Oncology, Oncology/tumor, Radiology, Research, Science \& Technology, Technology, tumor, Usage},
	pages = {1993--2024},
}

@article{li_automatic_2015,
	title = {Automatic {Segmentation} of {Liver} {Tumor} in {CT} {Images} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {3},
	copyright = {COPYRIGHT 2015 Modern Science Publishers},
	issn = {2327-5219},
	abstract = {Liver tumors segmentation from computed tomography (CT) images is an essential task for diagnosis and treatments of liver cancer. However, it is difficult owing to the variability of appearances, fuzzy boundaries, heterogeneous densities, shapes and sizes of lesions. In this paper, an automatic method based on convolutional neural networks (CNNs) is presented to segment lesions from CT images. The CNNs is one of deep learning models with some convolutional filters which can learn hierarchical features from data. We compared the CNNs model to popular machine learning algorithms: AdaBoost, Random Forests (RF), and support vector machine (SVM). These classifiers were trained by handcrafted features containing mean, variance, and contextual features. Experimental evaluation was performed on 30 portal phase enhanced CT images using leave-one-out cross validation. The average Dice Similarity Coefficient (DSC), precision, and recall achieved of 80.06\% [+ or -] 1.63\%, 82.67\% [+ or -] 1.43\%, and 84.34\% [+ or -] 1.61\%, respectively. The results show that the CNNs method has better performance than other methods and is promising in liver tumor segmentation. Keywords Liver Tumor Segmentation, Convolutional Neural Networks, Deep Learning, CT Image},
	language = {eng},
	number = {11},
	journal = {Journal of computer and communications},
	author = {Li, Wen and Jia, Fucang and Hu, Qingmao},
	year = {2015},
	note = {Publisher: Modern Science Publishers},
	keywords = {CT imaging, Data mining, Diagnosis, Image segmentation, Liver cancer, Machine learning, Methods, Neural networks, Research, Usage},
	pages = {146--151},
}

@article{cherukuri_learning_2018,
	title = {Learning {Based} {Segmentation} of {CT} {Brain} {Images}: {Application} to {Postoperative} {Hydrocephalic} {Scans}},
	volume = {65},
	issn = {0018-9294},
	language = {eng},
	number = {8},
	journal = {IEEE transactions on biomedical engineering},
	author = {Cherukuri, Venkateswararao and Ssenyonga, Peter and Warf, Benjamin C and Kulkarni, Abhaya V and Monga, Vishal and Schiff, Steven J},
	year = {2018},
	keywords = {Algorithms, Biomedical, Brain, Brain - diagnostic imaging, Computed tomography, Computer-Assisted - methods, CT image segmentation, Dictionaries, dictionary learning, Engineering, Hospitals, Humans, hydrocephalus, Hydrocephalus - diagnostic imaging, Image Interpretation, Image segmentation, Infant, Machine Learning, neurosurgery, Science \& Technology, subdural hematoma, Technology, Tomography, Training, volume, X-Ray Computed - methods},
	pages = {1871--1884},
}

@article{cheng_superpixel_2013,
	title = {Superpixel {Classification} {Based} {Optic} {Disc} and {Optic} {Cup} {Segmentation} for {Glaucoma} {Screening}},
	volume = {32},
	issn = {0278-0062},
	abstract = {Glaucoma is a chronic eye disease that leads to vision loss. As it cannot be cured, detecting the disease in time is important. Current tests using intraocular pressure (IOP) are not sensitive enough for population based glaucoma screening. Optic nerve head assessment in retinal fundus images is both more promising and superior. This paper proposes optic disc and optic cup segmentation using superpixel classification for glaucoma screening. In optic disc segmentation, histograms, and center surround statistics are used to classify each superpixel as disc or non-disc. A self-assessment reliability score is computed to evaluate the quality of the automated optic disc segmentation. For optic cup segmentation, in addition to the histograms and center surround statistics, the location information is also included into the feature space to boost the performance. The proposed segmentation methods have been evaluated in a database of 650 images with optic disc and optic cup boundaries manually marked by trained professionals. Experimental results show an average overlapping error of 9.5\% and 24.1\% in optic disc and optic cup segmentation, respectively. The results also show an increase in overlapping error as the reliability score is reduced, which justifies the effectiveness of the self-assessment. The segmented optic disc and optic cup are then used to compute the cup to disc ratio for glaucoma screening. Our proposed method achieves areas under curve of 0.800 and 0.822 in two data sets, which is higher than other methods. The methods can be used for segmentation and glaucoma screening. The self-assessment will be used as an indicator of cases with large errors and enhance the clinical deployment of the automatic segmentation and screening.},
	language = {eng},
	number = {6},
	journal = {IEEE transactions on medical imaging},
	author = {Cheng, Jun and Liu, Jiang and Xu, Yanwu and Yin, Fengshou and Wong, D. W. K and Tan, Ngan-Meng and Tao, Dacheng and Cheng, Ching-Yu and Aung, Tin and Wong, Tien Yin},
	year = {2013},
	keywords = {Adaptive optics, Area Under Curve, Biomedical, Computer Science, Computer-Assisted - methods, Databases, Deformable models, Diagnosis, Diagnostic imaging, Diagnostic Techniques, Electrical \& Electronic, Engineering, Factual, Glaucoma, Glaucoma - diagnosis, Glaucoma - pathology, Glaucoma screening, Histograms, Humans, Image color analysis, Image Interpretation, Image segmentation, Imaging Science \& Photographic Technology, Interdisciplinary Applications, Life Sciences \& Biomedicine, Methods, Nuclear Medicine \& Medical Imaging, Observations, Ophthalmological, optic cup segmentation, Optic disc, optic disc segmentation, Optic Disk - anatomy \& histology, Optical imaging, Optical sensors, Radiology, Reproducibility of Results, Research, Science \& Technology, Support Vector Machine, Technology},
	pages = {1019--1032},
}

@article{fu_joint_2018,
	title = {Joint {Optic} {Disc} and {Cup} {Segmentation} {Based} on {Multi}-{Label} {Deep} {Network} and {Polar} {Transformation}},
	volume = {37},
	copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
	issn = {0278-0062},
	abstract = {Glaucoma is a chronic eye disease that leads to irreversible vision loss. The cup to disc ratio (CDR) plays an important role in the screening and diagnosis of glaucoma. Thus, the accurate and automatic segmentation of optic disc (OD) and optic cup (OC) from fundus images is a fundamental task. Most existing methods segment them separately, and rely on hand-crafted visual feature from fundus images. In this paper, we propose a deep learning architecture, named M-Net, which solves the OD and OC segmentation jointly in a one-stage multi-label system. The proposed M-Net mainly consists of multi-scale input layer, U-shape convolutional network, side-output layer, and multi-label loss function. The multi-scale input layer constructs an image pyramid to achieve multiple level receptive field sizes. The U-shape convolutional network is employed as the main body network structure to learn the rich hierarchical representation, while the side-output layer acts as an early classifier that produces a companion local prediction map for different scale layers. Finally, a multi-label loss function is proposed to generate the final segmentation map. For improving the segmentation performance further, we also introduce the polar transformation, which provides the representation of the original image in the polar coordinate system. The experiments show that our M-Net system achieves state-of-the-art OD and OC segmentation result on ORIGA data set. Simultaneously, the proposed method also obtains the satisfactory glaucoma screening performances with calculated CDR value on both ORIGA and SCES datasets.},
	language = {eng},
	number = {7},
	journal = {IEEE transactions on medical imaging},
	author = {Fu, Huazhu and Cheng, Jun and Xu, Yanwu and Wong, Damon Wing Kee and Liu, Jiang and Cao, Xiaochun},
	year = {2018},
	keywords = {Biomedical, Biomedical optical imaging, Computer Science, Computer Science - Computer Vision and Pattern Recognition, Computer-Assisted - methods, cup to disc ratio, Databases, Deep Learning, Diagnostic Techniques, Electrical \& Electronic, Engineering, Factual, Glaucoma - diagnostic imaging, glaucoma screening, Humans, Image Interpretation, Image segmentation, Imaging Science \& Photographic Technology, Interdisciplinary Applications, Life Sciences \& Biomedicine, Machine learning, Nuclear Medicine \& Medical Imaging, Ophthalmological, optic cup segmentation, optic disc segmentation, Optic Disk - diagnostic imaging, Optical imaging, Optical losses, Radiology, Science \& Technology, Technology, Visualization},
	pages = {1597--1605},
}

@article{song_dual-channel_2017,
	title = {Dual-{Channel} {Active} {Contour} {Model} for {Megakaryocytic} {Cell} {Segmentation} in {Bone} {Marrow} {Trephine} {Histology} {Images}},
	volume = {64},
	issn = {0018-9294},
	language = {eng},
	number = {12},
	journal = {IEEE transactions on biomedical engineering},
	author = {Song, Tzu-Hsi and Sanchez, Victor and EIDaly, Hesham and Rajpoot, Nasir M},
	year = {2017},
	note = {Place: PISCATAWAY
Publisher: IEEE},
	keywords = {Active contours, Artificial intelligence, Biological system modeling, Biomedical, Biomedical imaging, bone marrow trephine biopsies, Bones, digital pathology, Engineering, Image color analysis, Image segmentation, Level set, megakaryocyte (MK) segmentation, Research, Science \& Technology, Technology},
	pages = {2913--2923},
}

@article{wang_central_2017,
	title = {Central focused convolutional neural networks: {Developing} a data-driven model for lung nodule segmentation},
	volume = {40},
	copyright = {2017 The Authors},
	issn = {1361-8415},
	abstract = {•A data-driven lung nodule segmentation method without involving shape hypothesis.•Two-branch convolutional neural networks extract both 3D and multi-scale 2D features.•A novel central pooling layer is proposed for feature selection.•We propose a weighted sampling method to solve imbalanced training label problem.•The method shows strong performance for segmenting juxtapleural nodules. [Display omitted] Accurate lung nodule segmentation from computed tomography (CT) images is of great importance for image-driven lung cancer analysis. However, the heterogeneity of lung nodules and the presence of similar visual characteristics between nodules and their surroundings make it difficult for robust nodule segmentation. In this study, we propose a data-driven model, termed the Central Focused Convolutional Neural Networks (CF-CNN), to segment lung nodules from heterogeneous CT images. Our approach combines two key insights: 1) the proposed model captures a diverse set of nodule-sensitive features from both 3-D and 2-D CT images simultaneously; 2) when classifying an image voxel, the effects of its neighbor voxels can vary according to their spatial locations. We describe this phenomenon by proposing a novel central pooling layer retaining much information on voxel patch center, followed by a multi-scale patch learning strategy. Moreover, we design a weighted sampling to facilitate the model training, where training samples are selected according to their degree of segmentation difficulty. The proposed method has been extensively evaluated on the public LIDC dataset including 893 nodules and an independent dataset with 74 nodules from Guangdong General Hospital (GDGH). We showed that CF-CNN achieved superior segmentation performance with average dice scores of 82.15\% and 80.02\% for the two datasets respectively. Moreover, we compared our results with the inter-radiologists consistency on LIDC dataset, showing a difference in average dice score of only 1.98\%.},
	language = {eng},
	journal = {Medical image analysis},
	author = {Wang, Shuo and Zhou, Mu and Liu, Zaiyi and Liu, Zhenyu and Gu, Dongsheng and Zang, Yali and Dong, Di and Gevaert, Olivier and Tian, Jie},
	year = {2017},
	note = {Place: AMSTERDAM
Publisher: Elsevier B.V},
	keywords = {Artificial Intelligence, Biomedical, Computer Science, Computer-aided diagnosis, Computer-Assisted - methods, Convolutional neural networks, CT imaging, Deep learning, Diagnosis, Engineering, Humans, Interdisciplinary Applications, Life Sciences \& Biomedicine, Lung cancer, Lung Neoplasms - diagnostic imaging, Lung nodule segmentation, Machine Learning, Neural networks, Neural Networks (Computer), Nuclear Medicine \& Medical Imaging, Radiology, Science \& Technology, Sensitivity and Specificity, Technology, Tomography, X-Ray Computed - methods},
	pages = {172--183},
}

@article{onishi_multiplanar_2019,
	title = {Multiplanar analysis for pulmonary nodule classification in {CT} images using deep convolutional neural network and generative adversarial networks},
	volume = {15},
	copyright = {CARS 2019},
	issn = {1861-6410},
	abstract = {Purpose Early detection and treatment of lung cancer holds great importance. However, pulmonary-nodule classification using CT images alone is difficult to realize. To address this concern, a method for pulmonary-nodule classification based on a deep convolutional neural network (DCNN) and generative adversarial networks (GAN) has previously been proposed by the authors. In that method, the said classification was performed exclusively using axial cross sections of pulmonary nodules. During actual medical-examination procedures, however, a comprehensive judgment can only be made via observation of various pulmonary-nodule cross sections. In the present study, a comprehensive analysis was performed by extending the application of the previously proposed DCNN- and GAN-based automatic classification method to multiple cross sections of pulmonary nodules. Methods Using the proposed method, CT images of 60 cases with confirmed pathological diagnosis by biopsy are analyzed. Firstly, multiplanar images of the pulmonary nodule are generated. Classification training was performed for three DCNNs. A certain pretraining was initially performed using GAN-generated nodule images. This was followed by fine-tuning of each pretrained DCNN using original nodule images provided as input. Results As a result of the evaluation, the specificity was 77.8\% and the sensitivity was 93.9\%. Additionally, the specificity was observed to have improved by 11.1\% without any reduction in the sensitivity, compared to our previous report. Conclusion This study reports development of a comprehensive analysis method to classify pulmonary nodules at multiple sections using GAN and DCNN. The effectiveness of the proposed discrimination method based on use of multiplanar images has been demonstrated to be improved compared to that realized in a previous study reported by the authors. In addition, the possibility of enhancing classification accuracy via application of GAN-generated images, instead of data augmentation, for pretraining even for medical datasets that contain relatively few images has also been demonstrated.},
	language = {eng},
	number = {1},
	journal = {International journal for computer assisted radiology and surgery},
	author = {Onishi, Yuya and Teramoto, Atsushi and Tsujimoto, Masakazu and Tsukamoto, Tetsuya and Saito, Kuniaki and Toyama, Hiroshi and Imaizumi, Kazuyoshi and Fujita, Hiroshi},
	year = {2019},
	publisher = {Springer International Publishing},
	pages = {173--178}
}

@article{chen_rethinking_2017,
	title = {Rethinking {Atrous} {Convolution} for {Semantic} {Image} {Segmentation}},
	copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
	language = {eng},
	author = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
	year = {2017},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	note = {\_eprint: https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
	pages = {1735--1780},
}

@article{chen_feature_2019,
	title = {Feature {Fusion} {Encoder} {Decoder} {Network} {For} {Automatic} {Liver} {Lesion} {Segmentation}},
	copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
	language = {eng},
	author = {Chen, Xueying and Zhang, Rong and Yan, Pingkun},
	year = {2019},
}

@article{christ_automatic_2016,
	title = {Automatic {Liver} and {Lesion} {Segmentation} in {CT} {Using} {Cascaded} {Fully} {Convolutional} {Neural} {Networks} and {3D} {Conditional} {Random} {Fields}},
	volume = {9901},
	url = {http://arxiv.org/abs/1610.02177},
	doi = {10.1007/978-3-319-46723-8_48},
	language = {en},
	urldate = {2022-03-01},
	journal = {arXiv:1610.02177 [cs]},
	author = {Christ, Patrick Ferdinand and Elshaer, Mohamed Ezzeldin A. and Ettlinger, Florian and Tatavarty, Sunil and Bickel, Marc and Bilic, Patrick and Rempfler, Markus and Armbruster, Marco and Hofmann, Felix and D'Anastasi, Melvin and Sommer, Wieland H. and Ahmadi, Seyed-Ahmad and Menze, Bjoern H.},
	year = {2016},
	note = {arXiv: 1610.02177},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {415--423},
	annote = {Comment: Accepted at MICCAI 2016. Source code available on https://github.com/IBBM/Cascaded-FCN},
}

@article{kaluva_2d-densely_2018,
	title = {{2D}-{Densely} {Connected} {Convolution} {Neural} {Networks} for automatic {Liver} and {Tumor} {Segmentation}},
	url = {http://arxiv.org/abs/1802.02182},
	abstract = {In this paper we propose a fully automatic 2-stage cascaded approach for segmentation of liver and its tumors in CT (Computed Tomography) images using densely connected fully convolutional neural network (DenseNet). We independently train liver and tumor segmentation models and cascade them for a combined segmentation of the liver and its tumor. The ﬁrst stage involves segmentation of liver and the second stage uses the ﬁrst stage’s segmentation results for localization of liver and henceforth tumor segmentations inside liver region. The liver model was trained on the down-sampled axial slices (256 × 256), whereas for the tumor model no down-sampling of slices was done, but instead it was trained on the CT axial slices windowed at three different Hounsﬁeld (HU) levels. On the test set our model achieved a global dice score of 0.923 and 0.625 on liver and tumor respectively. The computed tumor burden had an rmse of 0.044.},
	language = {en},
	urldate = {2022-03-01},
	journal = {arXiv:1802.02182 [cs]},
	author = {Kaluva, Krishna Chaitanya and Khened, Mahendra and Kori, Avinash and Krishnamurthi, Ganapathy},
	month = jan,
	year = {2018},
	note = {arXiv: 1802.02182},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Kaluva et al. - 2018 - 2D-Densely Connected Convolution Neural Networks f.pdf:/Users/jack/Zotero/storage/UEJ8KBGS/Kaluva et al. - 2018 - 2D-Densely Connected Convolution Neural Networks f.pdf:application/pdf},
}

@inproceedings{tang_dsl_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DSL}: {Automatic} {Liver} {Segmentation} with {Faster} {R}-{CNN} and {DeepLab}},
	copyright = {Springer Nature Switzerland AG 2018},
	isbn = {3-030-01420-7},
	language = {eng},
	booktitle = {Artificial {Neural} {Networks} and {Machine} {Learning} – {ICANN} 2018},
	publisher = {Springer International Publishing},
	author = {Tang, Wei and Zou, Dongsheng and Yang, Su and Shi, Jing},
	year = {2018},
	note = {ISSN: 0302-9743},
	keywords = {DeepLab, Detection, Faster R-CNN, Segmentation},
	pages = {137--147},
}

@incollection{feng_automatic_2018,
	address = {Singapore},
	series = {Lecture {Notes} in {Electrical} {Engineering}},
	title = {Automatic {Liver} and {Tumor} {Segmentation} of {CT} {Based} on {Cascaded} {U}-{Net}},
	copyright = {Springer Nature Singapore Pte Ltd. 2019},
	isbn = {9789811322907},
	language = {eng},
	booktitle = {Proceedings of 2018 {Chinese} {Intelligent} {Systems} {Conference}},
	publisher = {Springer Singapore},
	author = {Feng, Xiaorui and Wang, Chaoli and Cheng, Shuqun and Guo, Lei},
	year = {2018},
	note = {ISSN: 1876-1100},
	keywords = {Deep learning, FCN, Medical image, Segmentation, U-Net},
	pages = {155--164},
}

@inproceedings{mahapatra_efficient_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {Active} {Learning} for {Image} {Classification} and {Segmentation} {Using} a {Sample} {Selection} and {Conditional} {Generative} {Adversarial} {Network}},
	copyright = {Springer Nature Switzerland AG 2018},
	isbn = {978-3-030-00933-5},
	abstract = {Training robust deep learning (DL) systems for medical image classification or segmentation is challenging due to limited images covering different disease types and severity. We propose an active learning (AL) framework to select most informative samples and add to the training data. We use conditional generative adversarial networks (cGANs) to generate realistic chest xray images with different disease characteristics by conditioning its generation on a real image sample. Informative samples to add to the training set are identified using a Bayesian neural network. Experiments show our proposed AL framework is able to achieve state of the art performance by using about {\textbackslash}documentclass[12pt]minimal {\textbackslash}usepackageamsmath {\textbackslash}usepackagewasysym {\textbackslash}usepackageamsfonts {\textbackslash}usepackageamssymb {\textbackslash}usepackageamsbsy {\textbackslash}usepackagemathrsfs {\textbackslash}usepackageupgreek {\textbackslash}setlengthøddsidemargin-69pt {\textbackslash}begindocument\$\$35\%\$\${\textbackslash}enddocument of the full dataset, thus saving significant time and effort over conventional methods.},
	language = {eng},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2018},
	publisher = {Springer International Publishing},
	author = {Mahapatra, Dwarikanath and Bozorgtabar, Behzad and Thiran, Jean-Philippe and Reyes, Mauricio},
	year = {2018},
	note = {ISSN: 0302-9743},
	keywords = {Bayesian Neural Network (BNN), Conditional Generative Adversarial Networks, Efficient Active Learning, Medical Image Classification, Real Sample Images},
	pages = {580--588},
}
